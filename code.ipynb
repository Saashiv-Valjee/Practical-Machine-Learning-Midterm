{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "'''image_classes = np.loadtxt('/home/ap19060/Downloads/hirise ma'+'/labels-map-proj-v3.txt',dtype=str,delimiter='')\n",
    "df = pd.DataFrame(image_classes,columns=['filename','class'])\n",
    "\n",
    "x_train, y_train, x_test, y_test = getMNIST()\n",
    "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))\n",
    "x_test  = x_test.reshape((x_test.shape[0], 28, 28, 1))\n",
    "\n",
    "\n",
    "print(\"\\tN(train)             = \", len(x_train))\n",
    "print(\"\\tN(test)              = \", len(x_test))\n",
    "print(\"\\tTest data shape      = \", x_train.shape )\n",
    "print(\"\\tTrain data shape     = \", x_test.shape )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "image_classes = np.loadtxt('labels-map-proj-v3.txt',dtype=str,delimiter=' ')\n",
    "df =pd.DataFrame(image_classes,columns=['filename','class'])\n",
    "idg = ImageDataGenerator(rescale=1./255.) # normalize\n",
    "batchsize=10\n",
    "train_ds = idg.flow_from_dataframe(dataframe=df, directory='map-proj-v3',batch_size=batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_ds[0][0]))\n",
    "pixels = []\n",
    "classes = []\n",
    "for i in range(len(train_ds[0][0])):\n",
    "    pixels.append(train_ds[0][0][i])\n",
    "    classes.append(train_ds[0][1][i])\n",
    "test = 3\n",
    "plt.imshow(pixels[test])\n",
    "print(pixels[test].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Training configuration\n",
    "#\n",
    "BatchSize       = batchsize # be careful with this, if it is too big may crash!\n",
    "Nepochs         = 20\n",
    "DropoutValue    = 0.6\n",
    "\n",
    "\n",
    "xpix = pixels[test].shape[0]\n",
    "ypix = pixels[test].shape[1]\n",
    "zpix = pixels[test].shape[2] #colour\n",
    "print(xpix, ypix, zpix)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(xpix, ypix, zpix)))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.6))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "print(\"\\033[92mWill train a convolutional neural network on the MNIST data\\033[0m\")\n",
    "print(\"--------------------------------------------------------------------------------------------------------------\\n\\n\")\n",
    "print(\"Input data MNIST\")\n",
    "print(\"Dropout values       = \", DropoutValue)\n",
    "print(\"Leaky relu parameter =  0.1\")\n",
    "print(\"ValidationSplit      = \", ValidationSplit)\n",
    "print(\"BatchSize            = \", BatchSize)\n",
    "print(\"Nepochs              = \", Nepochs, \"\\n\")\n",
    "print(\"N(train)             = \", len(pixels))\n",
    "model.summary()\n",
    "\n",
    "# now specify the loss function - cross entropy\n",
    "loss_fn = keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "# now we can train the model to make predictions.\n",
    "#   Use the ADAM optimiser\n",
    "#   Specify the metrics to report as accuracy\n",
    "#   Specify the loss function (see above)\n",
    "# the fit step specifies the number of training epochs\n",
    "model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "history  = model.fit(train_ds, batch_size=10,epochs=Nepochs)\n",
    "\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
